{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa0eb4dc",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f44617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Callable, Union, List, Dict, Any, Tuple\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a652ec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bee238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7745d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from index_utils import IndexUtil\n",
    "from experiment_utils import ExperimentUtil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db783b10",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b911d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_model = KeyBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d3eab5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.local/lib/python3.8/site-packages/spacy/util.py:837: UserWarning: [W095] Model 'en_core_web_sm' (3.2.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.3.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee6002be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "pipeline_ner = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56916ee",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c31d259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENTS_SQUAD, QUESTIONS_SQUAD = ExperimentUtil.load_dataset('squad_10k')\n",
    "DOCUMENTS_SWIFT, QUESTIONS_SWIFT = ExperimentUtil.load_dataset('swift_ui')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9832487",
   "metadata": {},
   "source": [
    "### Extract keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9bb74cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_documents_keywords(documents):\n",
    "    texts = [doc['text'] for doc in documents]\n",
    "    keywords_base = kw_model.extract_keywords(docs=texts, vectorizer=KeyphraseCountVectorizer(), top_n=20)\n",
    "    keywords = [' '.join([keyword_pair[0] for keyword_pair in docs_keywords]) for docs_keywords in keywords_base]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bae10a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.local/lib/python3.8/site-packages/keybert/_model.py:130: UserWarning: Although extracting keywords for multiple documents is faster than iterating over single documents, it requires significantly more memory to hold all word embeddings. Use this at your own discretion!\n",
      "  warnings.warn(\n",
      "/home/daniel/.local/lib/python3.8/site-packages/spacy/util.py:837: UserWarning: [W095] Model 'en_core_web_sm' (3.2.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.3.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "10000it [00:06, 1519.60it/s]\n"
     ]
    }
   ],
   "source": [
    "SQUAD_KEYWORDS = prepare_documents_keywords(DOCUMENTS_SQUAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d02f4a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.local/lib/python3.8/site-packages/keybert/_model.py:130: UserWarning: Although extracting keywords for multiple documents is faster than iterating over single documents, it requires significantly more memory to hold all word embeddings. Use this at your own discretion!\n",
      "  warnings.warn(\n",
      "185it [00:00, 1426.31it/s]\n"
     ]
    }
   ],
   "source": [
    "SWIFT_KEYWORDS = prepare_documents_keywords(DOCUMENTS_SWIFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fdb423",
   "metadata": {},
   "source": [
    "### Extract lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14fdb08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    text_lemma = ''\n",
    "    for token in doc:\n",
    "        text_lemma = text_lemma + token.lemma_+' '\n",
    "    return text_lemma.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35cb898d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tom Hanks be a good actor as he love and have love playing .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_text('Tom Hanks was a good actor as he loves and had loved playing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "51479857",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQUAD_LEMMA = [lemmatize_text(doc['text']) for doc in DOCUMENTS_SQUAD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14199739",
   "metadata": {},
   "outputs": [],
   "source": [
    "SWIFT_LEMMA = [lemmatize_text(doc['text']) for doc in DOCUMENTS_SWIFT]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f9324",
   "metadata": {},
   "source": [
    "### Extract Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9992978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_named_entities(text):\n",
    "    named_entities = \" \".join([entity['word'] for entity in pipeline_ner(text)])\n",
    "    named_entities = named_entities.replace(' ##','')\n",
    "    named_entities = named_entities.replace('##','')\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78be82db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tom Hanks'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_named_entities('Tom Hanks was a good actor as he loves and had loved playing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "afc677fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8153d4e45d4f7ba782ed7682ef475b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SQUAD_NE = []\n",
    "for doc in tqdm(DOCUMENTS_SQUAD):\n",
    "    SQUAD_NE.append(extract_named_entities(doc['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "085f5dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee08fcd40b24d8c81b13e3f8acce564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/185 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SWIFT_NE = []\n",
    "for doc in tqdm(DOCUMENTS_SWIFT):\n",
    "    SWIFT_NE.append(extract_named_entities(doc['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f242650",
   "metadata": {},
   "source": [
    "### Override datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7050bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_full_dataset(dataset_name: str) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    documents_path, _ = ExperimentUtil.get_dataset_paths(dataset_name)\n",
    "    with open(documents_path) as json_file:\n",
    "        document_data = json.load(json_file)\n",
    "    return document_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2dcd40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQUAD_JSON = load_full_dataset('squad_10k')\n",
    "SWIFT_JSON = load_full_dataset('swift_ui')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52c453d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_fields(docs, keywords_list, lemma_list, ne_list):\n",
    "    for doc, keywords, lemma, ne in zip(docs, keywords_list, lemma_list, ne_list):\n",
    "        doc['keywords'] = keywords\n",
    "        doc['text_lemma'] = lemma\n",
    "        doc['ne'] = ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "759b9dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_fields(SWIFT_JSON['documents'], SWIFT_KEYWORDS, SWIFT_LEMMA, SWIFT_NE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "32cb0079",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_fields(SQUAD_JSON['documents'], SQUAD_KEYWORDS, SQUAD_LEMMA, SQUAD_NE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876a635a",
   "metadata": {},
   "source": [
    "### Save datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "20072c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/processed/squad_train_d10k_q1k_additional_fields/documents.json', 'w') as f:\n",
    "#     json.dump(SQUAD_JSON, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b0d069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/processed/swift-ui-course_additional_fields/documents.json', 'w') as f:\n",
    "#     json.dump(SWIFT_JSON, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
